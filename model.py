# CS121 Linear regression
# General purpose model representation and selection code

import numpy as np
import matplotlib.pylab as plt
import math
from asserts import assert_Xy, assert_Xbeta


#############################
#                           #
#  Our code: DO NOT MODIFY  #
#                           #
#############################


def prepend_ones_column(A):
    """
    Add a ones column to the left side of an array
    """
    ones_col = np.ones((A.shape[0], 1))
    return np.hstack([ones_col, A])


def linear_regression(X, y):
    """
    Compute linear regression. Finds model, beta, that minimizes
    X*beta - y in a least squared sense.

    Inputs:
        X: (2D Numpy array of floats) predictor/independent variables
        y: (1D Numpy array) dependent variable

    Returns: Numpy array beta, which is used only by apply_beta

    Examples
    --------
    >>> X = np.array([[5, 2], [3, 2], [6, 2.1], [7, 3]]) # predictors
    >>> y = np.array([5, 2, 6, 6]) # dependent
    >>> beta = linear_regression(X, y)  # compute the coefficients
    >>> beta
    array([ 1.20104895,  1.41083916, -1.6958042 ])
    >>> apply_beta(beta, X) # apply the function defined by beta
    array([ 4.86363636,  2.04195804,  6.1048951 ,  5.98951049])
    """
    assert_Xy(X, y, fname='linear_regression')

    X_with_ones = prepend_ones_column(X)

    # Do actual computation
    beta = np.linalg.lstsq(X_with_ones, y)[0]

    return beta


def apply_beta(beta, X):
    '''
    Apply beta, the function generated by linear_regression, to the
    specified values

    Inputs:
      beta: beta as returned by linear_regression
      X: (2D Numpy array of floats) predictor/independent variables

    Returns:
      result of applying beta to the data, as an array.

      Given:
        beta = array([B0, B1, B2,...BK])
        X = array([[x11, x12, ..., x0K],
                   [x21, x22, ..., x1K],
                   ...
                   [xN1, xN2, ..., xNK]])

      result will be:
        array([B0+B1*x11+B2*x12+...+BK*x1K,
               B0+B1*x21+B2*x22+...+BK*x2K,
               ...
               B0+B1*xN1+B2*xN2+...+BK*xNK])
    '''
    assert_Xbeta(X, beta, fname='apply_beta')

    # Add a column of ones
    X_incl_ones = prepend_ones_column(X)

    # Calculate X*beta
    yhat = np.dot(X_incl_ones, beta)
    return yhat


###############
#             #
#  Your code  #
#             #
###############


class Model(object):
    def __init__(self, dataset, pred_vars):
        '''
        Construct a data structure to hold the model.

        Inputs:
            dataset: an dataset instance
            pred_vars: a list of the indices for the columns used in
              the model.
        '''
        
        self.dataset = dataset
        ds = dataset
        self.pred_vars = pred_vars
        self.independent_array = ds.train[:, self.pred_vars]
        self.dependent_array = ds.train[:, ds.dvi]
        self.fit_equation = linear_regression(self.independent_array, \
        self.dependent_array)
        self.add_beta = apply_beta(self.fit_equation, self.independent_array)

        self.test_independent_array = ds.test[:, self.pred_vars]
        self.test_dependent_array = ds.test[:, ds.dvi]
        self.test_add_beta = apply_beta(self.fit_equation, \
        self.test_independent_array)


    #Task 1a
    def construct_p_models(self):
        '''
        Computes task 1a by printing all of the single variable models and
        their r2 values.
        '''
        
        ds = self.dataset

        for p in ds.pvi:
            md = Model(ds, [p])
            print(ds.column_labels[ds.dvi], "~",  md.fit_equation[0], \
            "+", md.fit_equation[1], "*" , ds.column_labels[p])
            print("R2:", calculate_r2(md.dependent_array, md.add_beta))
            print()


    #Task 1b
    def construct_single_model(self):
        '''
        Computes task 1b by printing a single model including all of the
        predictor variables and the total r2 value.
        '''

        ds = self.dataset
        md = Model(ds, ds.pvi)

        #stackoverflow.com/questions/12032214/print-new-output-on-same-line
        print(ds.column_labels[ds.dvi], "~", md.fit_equation[0], end = ' ')
        for p in ds.pvi:
            print("+", md.fit_equation[p+1], "*", ds.column_labels[p], \
            end = ' ')
        print()
        print("R2:", calculate_r2(md.dependent_array, md.add_beta))


    #Task 2
    def construct_bivariate_model(self):
        '''
        Computes task 2 by printing the bivariate model with
        the highest r2 value.
        '''
    
        bivariate_dict = {}
        ds = self.dataset

        for p in ds.pvi:
            for q in ds.pvi:
                if p >= q:
                    continue
                if p < q:
                    md = Model(ds, [p, q])
                    r2 = calculate_r2(md.dependent_array, md.add_beta)
                    bivariate_dict[r2] = [p, q]
        max_r2 = max(bivariate_dict.keys())
        max_bivariate = bivariate_dict[max_r2]
        
        md = Model(ds, max_bivariate)

        print(ds.column_labels[ds.dvi], "~", md.fit_equation[0], "+", \
        md.fit_equation[1], "*", ds.column_labels[max_bivariate[0]], "+", \
        md.fit_equation[2], "*", ds.column_labels[max_bivariate[1]])
        print("R2:", max_r2)


    #Task 3 Construction
    def construct_complex_model(self):
        '''
        Constructs a dictionary matching r2 values to their predictor
        variables and a list of lists of the predictor variables using 
        Backward elimination.
        '''

        ds = self.dataset
        p = len(ds.pvi)
        k_list = list(range(1, p + 1))
        var_dict = {}
        total_var_list = []

        for k in k_list:
            var_list = ds.pvi
            len_var_list = ds.dvi
            while len_var_list > k:
                model_dict = {}
                if k == ds.dvi:
                    break
                for v in range(len(var_list)):
                    var_list_2 = var_list[:v] + var_list[v + 1:]
                    md = Model(ds, var_list_2)
                    r2 = calculate_r2(md.dependent_array, md.add_beta)
                    model_dict[r2] = var_list_2
                max_r2 = max(model_dict.keys())
                var_list = model_dict[max_r2]
                var_dict[max_r2] = var_list
                total_var_list.append(var_list)
                len_var_list -= 1
                if len_var_list == k:
                    break

            var_list = ds.pvi
            md = Model(ds, var_list)
            r2 = calculate_r2(md.dependent_array, md.add_beta)
            var_dict[r2] = var_list
            total_var_list.append(var_list)

            return var_dict, total_var_list


    #Task 3 Print
    def print_complex_model(self):
        '''
        Computes task 3 by printing the model with the highest r2 value for
        each value in the length of the predictor variable list.
        '''

        ds = self.dataset
        md = Model(ds, ds.pvi)
        var_dict, total_var_list = md.construct_complex_model()
        total_var_list.sort(key = len)

        for t in total_var_list:
            md = Model(ds, t)
            print(ds.column_labels[ds.dvi], "~", md.fit_equation[0], \
            end = ' ')
            for v in t:
                print("+", md.fit_equation[t.index(v) + 1], "*", \
                ds.column_labels[v], end = ' ')
            print()
            print("R2:", calculate_r2(md.dependent_array, md.add_beta))
            print()


    #Task 4 Construction
    def select_best_k(self):
        '''
        Constructs a maximum adjusted r2 value, a maximum r2 value, and
        the list of predictor variables needed to compute task 4.
        '''

        ds = self.dataset
        md = Model(ds, ds.pvi)
        var_dict, total_var_list = md.construct_complex_model()
        r2_dict = {}

        for v in var_dict:
            md = Model(ds, var_dict[v])
            r2 = v
            N = ds.train.shape[0]
            K = len(var_dict[v])
            adj_r2 = calculate_adj_r2(r2, N, K)
            r2_dict[adj_r2] = r2

        max_adj_r2 = max(r2_dict)
        max_r2 = r2_dict[max_adj_r2]
        pred_vars = var_dict[max_r2]
        return max_adj_r2, max_r2, pred_vars


    #Task 4 Print
    def print_best_k(self):
        '''
        Computes task 4 by printing the complex model from task 3 with
        the highest r2 value and its r2 value adjusted for the number of
        predictor variables.
        '''
   
        ds = self.dataset
        md = Model(ds, ds.pvi)
        max_adj_r2, max_r2, pred_vars = md.select_best_k()
        md = Model(ds, pred_vars)

        print(ds.column_labels[ds.dvi], "~", md.fit_equation[0], end = ' ')
        for v in pred_vars:
            print("+", md.fit_equation[pred_vars.index(v) + 1], "*", \
            ds.column_labels[v], end = ' ')
        print()
        print("R2:", max_r2)
        print("Adjusted R2:", max_adj_r2)


    #Task 5
    def test_data(self):
        '''
        Computes task 5 by printing the model from task 4 with the r2 value
        of the testing data (not the training data, which was used to 
        compute everything else up to this point).
        '''

        ds = self.dataset
        md = Model(ds, ds.pvi)
        max_adj_r2, training_r2, pred_vars = md.select_best_k()
        md = Model(ds, pred_vars)
        testing_r2 = calculate_r2(md.test_dependent_array, md.test_add_beta)
         
        print(ds.column_labels[ds.dvi], "~", md.fit_equation[0], end = ' ')
        for v in pred_vars:
            print("+", md.fit_equation[pred_vars.index(v) + 1], "*", \
            ds.column_labels[v], end = ' ')
        print()
        print("Training R2:", training_r2)
        print("Testing R2:", testing_r2)


def calculate_variance(dep_array):
    '''
    Computes the variance of an array of numbers.

    Inputs:
        dep_array: An array of numbers.

    Returns: The variance of the array of numbers.
    '''

    mean = np.mean(dep_array)
    new_array = (dep_array - mean)**2
    variance = sum(new_array)

    return variance


def calculate_residual_variance(dep_array, beta_array):
    '''
    Computes the residual variance of an array of residuals subtracted by the
    array computed by the linear regression.

    Inputs:
        dep_array: An array of numbers.
        beta_array: The array calculated by the linear regression.

    Returns: The residual variance of the two arrays.
    '''

    residual_array = (dep_array - beta_array)**2
    residual_variance = sum(residual_array)
 
    return residual_variance
    

def calculate_r2(dep_array, beta_array):
    '''
    Computes the r2 value of 2 arrays.

    Inputs:
        dep_array: An array of numbers.
        beta_array: The array calculated by the linear regression.

    Returns: The r2 value.
    '''

    variance = calculate_variance(dep_array)
    residual_variance = calculate_residual_variance(dep_array, beta_array)

    R2 = 1 - (residual_variance / variance)
    
    return R2


def calculate_adj_r2(r2, N, K):
    '''
    Computes the r2 value adjusted for the number of predictor variables.

    Inputs:
        r2: (float) The calculated r2 value.
        N: (int) The sample size of the data.
        K: (int) The number of predictor variables.
    '''

    adj_r2 = r2 - (1 - r2) * (K / (N - K - 1))

    return adj_r2

